#!/usr/bin/env python3
"""
Test script for 06_Evaluation

Tests performance monitoring, quality evaluation, and evaluation pipeline.
"""

import os
import sys
import time
import logging

# Add project root to path
sys.path.append(os.path.join(os.path.dirname(__file__), ".."))

from app.rag._06_evaluation import (
    get_performance_monitor,
    record_search_performance,
    get_current_performance_stats,
    get_quality_evaluator,
    evaluate_search_quality,
    add_manual_relevance_judgments,
    get_evaluation_pipeline,
    run_full_evaluation,
    quick_evaluation,
)

# Configure logging
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(name)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)


def mock_search_function(query: str) -> dict:
    """Mock search function for testing."""
    # Simulate different response times and results
    import random

    time.sleep(random.uniform(0.02, 0.15))  # 20-150ms response time

    # Mock results based on query
    mock_results = []

    if "action" in query.lower():
        mock_results = [
            {
                "movie_id": "1",
                "title": "Die Hard",
                "overview": "Action thriller",
                "genres": ["Action"],
                "scores": {"final": 0.85},
            },
            {
                "movie_id": "2",
                "title": "Terminator",
                "overview": "Sci-fi action",
                "genres": ["Action", "Sci-Fi"],
                "scores": {"final": 0.82},
            },
            {
                "movie_id": "3",
                "title": "Mad Max",
                "overview": "Post-apocalyptic action",
                "genres": ["Action"],
                "scores": {"final": 0.78},
            },
        ]
    elif "comedy" in query.lower():
        mock_results = [
            {
                "movie_id": "4",
                "title": "The Hangover",
                "overview": "Comedy about a bachelor party",
                "genres": ["Comedy"],
                "scores": {"final": 0.88},
            },
            {
                "movie_id": "5",
                "title": "Superbad",
                "overview": "Teen comedy",
                "genres": ["Comedy"],
                "scores": {"final": 0.85},
            },
            {
                "movie_id": "6",
                "title": "Anchorman",
                "overview": "Comedy about a news anchor",
                "genres": ["Comedy"],
                "scores": {"final": 0.83},
            },
        ]
    elif "animated" in query.lower() or "kids" in query.lower():
        mock_results = [
            {
                "movie_id": "7",
                "title": "Toy Story",
                "overview": "Animated adventure",
                "genres": ["Animation", "Family"],
                "scores": {"final": 0.92},
            },
            {
                "movie_id": "8",
                "title": "Finding Nemo",
                "overview": "Underwater adventure",
                "genres": ["Animation", "Family"],
                "scores": {"final": 0.90},
            },
        ]
    elif query.strip() == "" or query.lower() in ["a", "the the the"]:
        # Edge cases
        mock_results = []
    elif "nonexistent" in query.lower() or "2050" in query:
        # No results for impossible queries
        mock_results = []
    else:
        # Generic results
        mock_results = [
            {
                "movie_id": "9",
                "title": "Generic Movie",
                "overview": "A movie",
                "genres": ["Drama"],
                "scores": {"final": 0.65},
            },
        ]

    return {
        "results": mock_results,
        "total_found": len(mock_results),
        "search_time": random.uniform(0.02, 0.15),
        "query_info": {"original_query": query},
    }


def test_performance_monitoring():
    """Test performance monitoring functionality."""
    print("\n" + "=" * 60)
    print("ğŸ“Š Testing Performance Monitoring")
    print("=" * 60)

    try:
        # Get performance monitor
        monitor = get_performance_monitor()

        print("ğŸ§ª Testing performance monitoring features:")

        # Test 1: Record manual metrics
        print(f"\nğŸ“ Test 1: Recording Manual Metrics")
        monitor.record_metric("test_latency", 50.0, "ms")
        monitor.record_metric("test_accuracy", 0.85, "ratio")
        print("   âœ… Manual metrics recorded successfully")

        # Test 2: Record search performance
        print(f"\nğŸ“ Test 2: Recording Search Performance")
        record_search_performance("test query", 0.1, 5, 0.02)
        record_search_performance("another query", 0.05, 8, 0.01)
        print("   âœ… Search performance metrics recorded")

        # Test 3: Get metric statistics
        print(f"\nğŸ“ Test 3: Getting Metric Statistics")
        stats = monitor.get_metric_stats("test_latency")
        if "mean" in stats:
            print(
                f"   âœ… Latency stats: mean={stats['mean']:.1f}{stats.get('unit', '')}"
            )
        else:
            print(f"   âš ï¸  Stats format: {stats}")

        # Test 4: Get search performance summary
        print(f"\nğŸ“ Test 4: Getting Search Performance Summary")
        summary = monitor.get_search_performance_summary(time_window_minutes=5)
        if isinstance(summary, dict) and "error" not in summary:
            search_stats = summary.get("summary", {})
            print(
                f"   âœ… Search summary: {search_stats.get('total_searches', 0)} searches"
            )
        else:
            print(f"   âš ï¸  No search data or error: {summary}")

        # Test 5: System health check
        print(f"\nğŸ“ Test 5: System Health Check")
        health = monitor.get_system_health()
        if health.get("health_status"):
            print(f"   âœ… System health: {health['health_status']}")
            print(
                f"   ğŸ’¾ Memory usage: {health['system_resources']['memory_percent']:.1f}%"
            )
            print(f"   ğŸ”¥ CPU usage: {health['system_resources']['cpu_percent']:.1f}%")
        else:
            print(f"   âŒ Health check failed")

        # Test 6: Performance benchmark
        print(f"\nğŸ“ Test 6: Performance Benchmark")
        test_queries = ["action", "comedy", "sci-fi"]
        benchmark = monitor.benchmark_search_performance(
            mock_search_function, test_queries, iterations=2
        )

        if benchmark.get("performance_grade"):
            print(f"   âœ… Benchmark completed")
            print(f"   ğŸ“Š Performance grade: {benchmark['performance_grade']}")
            print(
                f"   â±ï¸  Avg search time: {benchmark['overall_stats']['avg_search_time']:.3f}s"
            )
            print(
                f"   ğŸ¯ Success rate: {benchmark['overall_stats']['overall_success_rate']:.1%}"
            )
        else:
            print(f"   âŒ Benchmark failed")

        print(f"\nğŸ“Š Performance Monitoring: 6/6 tests completed")
        return True

    except Exception as e:
        print(f"âŒ Performance monitoring test failed: {str(e)}")
        import traceback

        traceback.print_exc()
        return False


def test_quality_evaluation():
    """Test quality evaluation functionality."""
    print("\n" + "=" * 60)
    print("ğŸ¯ Testing Quality Evaluation")
    print("=" * 60)

    try:
        # Get quality evaluator
        evaluator = get_quality_evaluator()

        print("ğŸ§ª Testing quality evaluation features:")

        # Test 1: Add manual relevance judgments
        print(f"\nğŸ“ Test 1: Adding Manual Relevance Judgments")
        add_manual_relevance_judgments(
            "action movies",
            {
                "1": 0.9,  # Die Hard - highly relevant
                "2": 0.8,  # Terminator - relevant
                "4": 0.1,  # Comedy - not relevant
            },
        )
        print("   âœ… Manual judgments added")

        # Test 2: Evaluate search quality
        print(f"\nğŸ“ Test 2: Evaluating Search Quality")
        mock_results = mock_search_function("action movies")["results"]
        evaluation = evaluate_search_quality("action movies", mock_results)

        if evaluation.precision_at_k:
            print(f"   âœ… Quality evaluation completed")
            print(f"   ğŸ“Š Precision@5: {evaluation.precision_at_k.get(5, 0.0):.3f}")
            print(f"   ğŸ“Š NDCG@5: {evaluation.ndcg_at_k.get(5, 0.0):.3f}")
            print(f"   ğŸ“Š MAP score: {evaluation.map_score:.3f}")
            print(f"   ğŸ“Š MRR score: {evaluation.mrr_score:.3f}")
        else:
            print(f"   âŒ Quality evaluation failed")

        # Test 3: Evaluate multiple queries
        print(f"\nğŸ“ Test 3: Evaluating Multiple Queries")
        query_results = {
            "action movies": mock_search_function("action movies")["results"],
            "comedy films": mock_search_function("comedy films")["results"],
        }

        # Add judgments for comedy
        add_manual_relevance_judgments(
            "comedy films",
            {
                "4": 0.9,  # Comedy - highly relevant
                "5": 0.8,  # Comedy - relevant
                "1": 0.1,  # Action - not relevant
            },
        )

        # Convert to SearchResult format for evaluation
        from app.rag._06_evaluation.quality_metrics import SearchResult

        search_result_sets = {}
        for query, results in query_results.items():
            search_results = []
            for i, result in enumerate(results):
                search_result = SearchResult(
                    document_id=str(result.get("movie_id", result.get("id", i))),
                    title=result.get("title", "Unknown"),
                    score=result.get("scores", {}).get(
                        "final", result.get("score", 0.0)
                    ),
                    rank=i + 1,
                    content=result.get("overview", ""),
                    metadata=result.get("metadata", {}),
                )
                search_results.append(search_result)
            search_result_sets[query] = search_results

        multi_evaluation = evaluator.evaluate_query_set(search_result_sets)

        if multi_evaluation.get("aggregate_metrics"):
            print(f"   âœ… Multi-query evaluation completed")
            agg = multi_evaluation["aggregate_metrics"]
            print(
                f"   ğŸ“Š Avg Precision@5: {agg.get('precision_at_k', {}).get(5, 0.0):.3f}"
            )
            print(f"   ğŸ“Š Avg NDCG@5: {agg.get('ndcg_at_k', {}).get(5, 0.0):.3f}")
        else:
            print(f"   âŒ Multi-query evaluation failed")

        # Test 4: Generate quality report
        print(f"\nğŸ“ Test 4: Generating Quality Report")
        if multi_evaluation.get("aggregate_metrics"):
            report = evaluator.generate_evaluation_report(multi_evaluation)

            if report.get("overall_grade"):
                print(f"   âœ… Quality report generated")
                print(f"   ğŸ“Š Overall grade: {report['overall_grade']}")
                print(
                    f"   ğŸ’¡ Recommendations: {len(report.get('recommendations', []))}"
                )
            else:
                print(f"   âŒ Report generation failed")
        else:
            print(f"   âš ï¸  Skipping report (no aggregate data)")

        print(f"\nğŸ“Š Quality Evaluation: 4/4 tests completed")
        return True

    except Exception as e:
        print(f"âŒ Quality evaluation test failed: {str(e)}")
        import traceback

        traceback.print_exc()
        return False


def test_evaluation_pipeline():
    """Test evaluation pipeline functionality."""
    print("\n" + "=" * 60)
    print("ğŸ”— Testing Evaluation Pipeline")
    print("=" * 60)

    try:
        # Get evaluation pipeline
        pipeline = get_evaluation_pipeline()

        print("ğŸ§ª Testing evaluation pipeline features:")

        # Test 1: Quick evaluation
        print(f"\nğŸ“ Test 1: Quick Evaluation")
        quick_results = quick_evaluation(mock_search_function)

        if quick_results.get("summary"):
            summary = quick_results["summary"]
            print(f"   âœ… Quick evaluation completed")
            print(f"   ğŸ“Š Success rate: {summary['success_rate']:.1%}")
            print(f"   â±ï¸  Avg response time: {summary['avg_response_time']:.3f}s")
            print(
                f"   ğŸ¯ Successful queries: {summary['successful_queries']}/{summary['total_queries']}"
            )
        else:
            print(f"   âŒ Quick evaluation failed")

        # Test 2: Performance evaluation component
        print(f"\nğŸ“ Test 2: Performance Evaluation Component")
        perf_results = pipeline._run_performance_evaluation(mock_search_function)

        if perf_results.get("benchmark"):
            benchmark = perf_results["benchmark"]
            print(f"   âœ… Performance evaluation completed")
            print(
                f"   ğŸ“Š Performance grade: {benchmark.get('performance_grade', 'Unknown')}"
            )
            stats = benchmark.get("overall_stats", {})
            print(f"   â±ï¸  Avg search time: {stats.get('avg_search_time', 0):.3f}s")
            print(f"   ğŸ¯ Success rate: {stats.get('overall_success_rate', 0):.1%}")
        else:
            print(f"   âŒ Performance evaluation failed")

        # Test 3: Quality evaluation component
        print(f"\nğŸ“ Test 3: Quality Evaluation Component")
        quality_results = pipeline._run_quality_evaluation(mock_search_function)

        if quality_results.get("quality_report"):
            report = quality_results["quality_report"]
            print(f"   âœ… Quality evaluation completed")
            print(f"   ğŸ“Š Quality grade: {report.get('overall_grade', 'Unknown')}")
            metrics = report.get("key_metrics", {})
            print(f"   ğŸ“ˆ Avg NDCG@5: {metrics.get('avg_ndcg_at_5', 0):.3f}")
        else:
            print(f"   âš ï¸  Quality evaluation returned: {quality_results}")

        # Test 4: Scenario testing
        print(f"\nğŸ“ Test 4: Scenario Testing")
        scenario_results = pipeline._run_scenario_testing(mock_search_function)

        if scenario_results:
            print(f"   âœ… Scenario testing completed")
            for scenario_name, scenario in scenario_results.items():
                success_rate = scenario.get("success_rate", 0)
                avg_time = scenario.get("avg_response_time", 0)
                print(
                    f"   ğŸ“Š {scenario_name}: {success_rate:.1%} success, {avg_time:.3f}s avg"
                )
        else:
            print(f"   âŒ Scenario testing failed")

        # Test 5: Comprehensive evaluation (shorter version)
        print(f"\nğŸ“ Test 5: Comprehensive Evaluation (Quick)")
        # Use a minimal test to avoid long runtime
        pipeline.test_scenarios["minimal_test"] = {
            "queries": ["action", "comedy"],
            "description": "Minimal test for pipeline",
        }

        # Override the original scenarios temporarily
        original_scenarios = pipeline.test_scenarios.copy()
        pipeline.test_scenarios = {
            "minimal_test": pipeline.test_scenarios["minimal_test"]
        }

        try:
            comprehensive_results = pipeline.run_comprehensive_evaluation(
                mock_search_function, enable_monitoring=False
            )

            if comprehensive_results.get("overall_assessment"):
                assessment = comprehensive_results["overall_assessment"]
                print(f"   âœ… Comprehensive evaluation completed")
                print(f"   ğŸ“Š Overall grade: {assessment['overall_grade']}")
                print(f"   ğŸ’ª Strengths: {len(assessment.get('strengths', []))}")
                print(f"   âš ï¸  Weaknesses: {len(assessment.get('weaknesses', []))}")
                print(
                    f"   ğŸ’¡ Recommendations: {len(assessment.get('recommendations', []))}"
                )
            else:
                print(f"   âŒ Comprehensive evaluation failed")

        finally:
            # Restore original scenarios
            pipeline.test_scenarios = original_scenarios

        print(f"\nğŸ“Š Evaluation Pipeline: 5/5 tests completed")
        return True

    except Exception as e:
        print(f"âŒ Evaluation pipeline test failed: {str(e)}")
        import traceback

        traceback.print_exc()
        return False


def test_integration_with_search():
    """Test integration with actual search system."""
    print("\n" + "=" * 60)
    print("ğŸ”— Testing Integration with Search System")
    print("=" * 60)

    try:
        print("ğŸ§ª Testing integration with real search components:")

        # Test 1: Integration with hybrid search
        print(f"\nğŸ“ Test 1: Integration with Hybrid Search")
        try:
            from app.rag._05_retrieval import hybrid_search

            # Test basic integration
            results = quick_evaluation(lambda q: hybrid_search(q, limit=5))

            if results.get("summary"):
                summary = results["summary"]
                print(f"   âœ… Hybrid search integration successful")
                print(f"   ğŸ“Š Success rate: {summary['success_rate']:.1%}")
                print(f"   â±ï¸  Avg response time: {summary['avg_response_time']:.3f}s")
            else:
                print(f"   âŒ Hybrid search integration failed")

        except ImportError as e:
            print(f"   âš ï¸  Hybrid search not available: {str(e)}")
        except Exception as e:
            print(f"   âŒ Hybrid search integration error: {str(e)}")

        # Test 2: Performance monitoring integration
        print(f"\nğŸ“ Test 2: Performance Monitoring Integration")
        monitor = get_performance_monitor()

        # Record some search metrics
        record_search_performance("integration test", 0.05, 3)
        record_search_performance("another test", 0.08, 5)

        stats = get_current_performance_stats()
        if stats.get("system_health"):
            print(f"   âœ… Performance monitoring integration successful")
            health_status = stats["system_health"].get("health_status", "Unknown")
            print(f"   ğŸ’š System health: {health_status}")
        else:
            print(f"   âŒ Performance monitoring integration failed")

        # Test 3: Quality evaluation with real data format
        print(f"\nğŸ“ Test 3: Quality Evaluation with Real Data Format")

        # Mock real search result format
        real_format_results = [
            {
                "movie_id": "1",
                "title": "The Matrix",
                "overview": "A computer programmer discovers reality is a simulation",
                "genres": ["Action", "Sci-Fi"],
                "year": 1999,
                "scores": {"final": 0.92, "semantic": 0.85, "keyword": 0.7},
                "rank": 1,
            },
            {
                "movie_id": "2",
                "title": "Blade Runner",
                "overview": "A blade runner hunts synthetic humans",
                "genres": ["Sci-Fi", "Thriller"],
                "year": 1982,
                "scores": {"final": 0.88, "semantic": 0.82, "keyword": 0.6},
                "rank": 2,
            },
        ]

        evaluation = evaluate_search_quality("sci-fi movies", real_format_results)

        if evaluation.precision_at_k:
            print(f"   âœ… Real data format evaluation successful")
            print(f"   ğŸ“Š Precision@5: {evaluation.precision_at_k.get(5, 0.0):.3f}")
            print(f"   ğŸ“Š Results analyzed: {evaluation.results_analyzed}")
        else:
            print(f"   âŒ Real data format evaluation failed")

        print(f"\nğŸ“Š Integration Tests: 3/3 tests completed")
        return True

    except Exception as e:
        print(f"âŒ Integration test failed: {str(e)}")
        import traceback

        traceback.print_exc()
        return False


def main():
    """Run all evaluation system tests."""
    print("ğŸš€ CineRAG 06_Evaluation Test Suite")
    print("Testing performance monitoring, quality evaluation, and evaluation pipeline")

    # Track test results
    test_results = {}

    # Run tests
    test_results["performance_monitoring"] = test_performance_monitoring()
    test_results["quality_evaluation"] = test_quality_evaluation()
    test_results["evaluation_pipeline"] = test_evaluation_pipeline()
    test_results["integration"] = test_integration_with_search()

    # Print summary
    print("\n" + "=" * 60)
    print("ğŸ“‹ TEST SUMMARY")
    print("=" * 60)

    total_tests = len(test_results)
    passed_tests = sum(test_results.values())

    print(f"âœ… Passed: {passed_tests}/{total_tests} test suites")

    for test_name, result in test_results.items():
        status = "âœ… PASS" if result else "âŒ FAIL"
        print(f"   {status} - {test_name.replace('_', ' ').title()} Test")

    if passed_tests == total_tests:
        print("\nğŸ‰ All tests passed! Evaluation system is ready for production.")
        print("ğŸ”¥ 06_Evaluation implementation complete!")
        print("ğŸ¯ RAG Pipeline now includes comprehensive evaluation capabilities!")
        return True
    else:
        print(
            f"\nâš ï¸  {total_tests - passed_tests} test suites failed. Please check the logs above."
        )
        return False


if __name__ == "__main__":
    success = main()
    sys.exit(0 if success else 1)
